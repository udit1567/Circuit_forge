# cloud_llm API Reference

## Index

- Class `CloudLLM`
- Class `CloudModel`

---

## `CloudLLM` class

```python
class CloudLLM(api_key: str, model: Union[str, CloudModel], system_prompt: str, temperature: Optional[float], timeout: int)
```

A Brick for interacting with cloud-based Large Language Models (LLMs).

This class wraps LangChain functionality to provide a simplified, unified interface
for chatting with models like Claude, GPT, and Gemini. It supports both synchronous
'one-shot' responses and streaming output, with optional conversational memory.

### Parameters

- **api_key** (*str*): The API access key for the target LLM service. Defaults to the
'API_KEY' environment variable.
- **model** (*Union[str, CloudModel]*): The model identifier. Accepts a `CloudModel`
enum member (e.g., `CloudModel.OPENAI_GPT`) or its corresponding raw string
value (e.g., `'gpt-4o-mini'`). Defaults to `CloudModel.ANTHROPIC_CLAUDE`.
- **system_prompt** (*str*): A system-level instruction that defines the AI's persona
and constraints (e.g., "You are a helpful assistant"). Defaults to empty.
- **temperature** (*Optional[float]*): The sampling temperature between 0.0 and 1.0.
Higher values make output more random/creative; lower values make it more
deterministic. Defaults to 0.7.
- **timeout** (*int*): The maximum duration in seconds to wait for a response before
timing out. Defaults to 30.

### Raises

- **ValueError**: If `api_key` is not provided (empty string).

### Methods

#### `with_memory(max_messages: int)`

Enables conversational memory for this instance.

Configures the Brick to retain a window of previous messages, allowing the
AI to maintain context across multiple interactions.

##### Parameters

- **max_messages** (*int*): The maximum number of messages (user + AI) to keep
in history. Older messages are discarded. Set to 0 to disable memory.
Defaults to 10.

##### Returns

- (*CloudLLM*): The current instance, allowing for method chaining.

#### `chat(message: str)`

Sends a message to the AI and blocks until the complete response is received.

This method automatically manages conversation history if memory is enabled.

##### Parameters

- **message** (*str*): The input text prompt from the user.

##### Returns

- (*str*): The complete text response generated by the AI.

##### Raises

- **RuntimeError**: If the internal chain is not initialized or if the API request fails.

#### `chat_stream(message: str)`

Sends a message to the AI and yields response tokens as they are generated.

This allows for processing or displaying the response in real-time (streaming).
The generation can be interrupted by calling `stop_stream()`.

##### Parameters

- **message** (*str*): The input text prompt from the user.

##### Returns

- (*str*): Chunks of text (tokens) from the AI response.

##### Raises

- **RuntimeError**: If the internal chain is not initialized or if the API request fails.
- **AlreadyGenerating**: If a streaming session is already active.

#### `stop_stream()`

Signals the active streaming generation to stop.

This sets an internal flag that causes the `chat_stream` iterator to break
early. It has no effect if no stream is currently running.

#### `clear_memory()`

Clears the conversational memory history.

Resets the stored context. This is useful for starting a new conversation
topic without previous context interfering. Only applies if memory is enabled.


---

## `CloudModel` class

```python
class CloudModel()
```

